{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88bcd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a3a6d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 (Conv1d): input shape (1, 260), output shape (2, 85)\n",
      "Layer 2 (MaxPool): input shape (2, 85), output shape (2, 42)\n",
      "Layer 3 (Conv1d): input shape (2, 42), output shape (4, 19)\n",
      "Layer 4 (MaxPool): input shape (4, 19), output shape (4, 9)\n",
      "Layer 5 (Flatten): input shape (4, 9), output shape 36\n"
     ]
    }
   ],
   "source": [
    "layers = {\n",
    "    'layer1': {'type': 'Conv1d', 'kernel_size': 7, 'stride': 3, 'out_channels': 2},\n",
    "    'layer2': {'type': 'MaxPool', 'kernel_size': 2, 'stride': 2},\n",
    "    'layer3': {'type': 'Conv1d', 'kernel_size': 5, 'stride': 2, 'out_channels': 4},\n",
    "    'layer4': {'type': 'MaxPool', 'kernel_size': 2, 'stride': 2},\n",
    "    'layer5': {'type': 'Flatten'}\n",
    "}\n",
    "\n",
    "def layer_calculation(layers, input_shape, channels=1):\n",
    "    input_shape = input_shape\n",
    "    channels = channels\n",
    "    for i in np.arange(1, len(layers)+1):\n",
    "        layer = layers[f\"layer{i}\"]\n",
    "        if layer['type'] == 'Conv1d':\n",
    "            kernel_size = layer['kernel_size']\n",
    "            stride = layer['stride']\n",
    "            out_channels = layer['out_channels']\n",
    "            \n",
    "            output_length = (input_shape - kernel_size) // stride + 1\n",
    "            output_shape = (out_channels, output_length)\n",
    "            print(f\"Layer {i} ({layers[f'layer{i}']['type']}): input shape ({channels}, {input_shape}), output shape {output_shape}\")\n",
    "            input_shape = output_length\n",
    "            channels = out_channels\n",
    "            \n",
    "        # Add more layer types as needed\n",
    "        if layer['type'] == 'MaxPool':\n",
    "            kernel_size = layer['kernel_size']\n",
    "            stride = layer['stride']\n",
    "            \n",
    "            output_length = (input_shape - kernel_size) // stride + 1\n",
    "            output_shape = (channels, output_length)\n",
    "            print(f\"Layer {i} ({layers[f'layer{i}']['type']}): input shape ({channels}, {input_shape}), output shape ({channels}, {output_length})\")\n",
    "            input_shape = output_length\n",
    "            \n",
    "        if layer['type'] == 'Flatten':\n",
    "            print(f\"Layer {i} ({layers[f'layer{i}']['type']}): input shape ({channels}, {input_shape}), output shape {channels * input_shape}\")\n",
    "            input_shape = input_shape\n",
    "        \n",
    "layer_calculation(layers, input_shape = 260)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea897aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv1d(1, 2, kernel_size=(7,), stride=(3,))\n",
       "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(2, 4, kernel_size=(5,), stride=(2,))\n",
       "  (maxpool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (output_binary): Linear(in_features=36, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure, layers\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layers\n",
    "        self.conv1 = nn.Conv1d(input_size[0], 2, kernel_size = 7, stride= 3, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size = 2, stride = 2, padding = 0) # max pooling layer\n",
    "        self.conv2 = nn.Conv1d(2, 4, kernel_size = 5, stride = 2, padding = 0) # average pooling layer\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size = 2, stride = 2, padding = 0) # max pooling layer\n",
    "        self.flatten = nn.Flatten() # flatten layer\n",
    "        self.output_binary = nn.Linear(36, 1) # output layer for binary classification\n",
    "        \n",
    "        # activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.output_binary(x)\n",
    "        return x\n",
    "    \n",
    "# data loader\n",
    "class DataLoaderAcc(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "# create model\n",
    "model = CNN(input_size=(1, 260))\n",
    "\n",
    "# define criteria\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8e96bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (31523, 1, 260), with mean: 0.00000, and std: 1.00000\n",
      "Test data shape: (6857, 1, 260), with mean: -0.00002, and std: 1.07414\n",
      "Train labels shape: (31523, 1), with distribution: (array([0., 1.], dtype=float32), array([24309,  7214]))\n",
      "Test labels shape: (6857, 1), with distribution: (array([0., 1.], dtype=float32), array([5143, 1714]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_978272/442329957.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train = np.reshape(train_data['label'].replace('locomotion', 1).replace('no locomotion', 0).values, (-1, 1))\n",
      "/tmp/ipykernel_978272/442329957.py:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_test = np.reshape(test_data['label'].replace('locomotion', 1).replace('no locomotion', 0).values, (-1, 1))\n"
     ]
    }
   ],
   "source": [
    "### laod data\n",
    "root_dir = \"/home/elias/2025/sshfs_mounter_2025/data_elias/ECSS_2026/raw_lab\"\n",
    "\n",
    "### get data from csv files\n",
    "df_list = []\n",
    "for f in os.listdir(root_dir):\n",
    "    df_list.append(pd.read_csv(os.path.join(root_dir, f)))\n",
    "    \n",
    "\n",
    "# use 20% of dataframes for testing\n",
    "test_size = int(0.2 * len(df_list))\n",
    "test_list = []\n",
    "for _ in range(test_size):\n",
    "    test_list.append(df_list.pop(np.random.randint(0, len(df_list))))\n",
    "    \n",
    "train_data = pd.concat(df_list, ignore_index=True)\n",
    "test_data = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "# shuffle datasets\n",
    "train_data = shuffle(train_data, random_state=42).reset_index(drop=True)\n",
    "test_data = shuffle(test_data, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# data reshaping\n",
    "y_train = np.reshape(train_data['label'].replace('locomotion', 1).replace('no locomotion', 0).values, (-1, 1))\n",
    "X_train = np.reshape(train_data.drop(columns=['label']).values, (-1, 1, 260))\n",
    "y_test = np.reshape(test_data['label'].replace('locomotion', 1).replace('no locomotion', 0).values, (-1, 1))\n",
    "X_test = np.reshape(test_data.drop(columns=['label']).values, (-1, 1, 260))\n",
    "\n",
    "# normalize\n",
    "train_mean = np.mean(X_train)\n",
    "train_sd = np.std(X_train)\n",
    "\n",
    "X_train = (X_train - train_mean) / train_sd\n",
    "X_test = (X_test - train_mean) / train_sd\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "print(f\"Train data shape: {X_train.shape}, with mean: {np.mean(X_train):.5f}, and std: {np.std(X_train):.5f}\")\n",
    "print(f\"Test data shape: {X_test.shape}, with mean: {np.mean(X_test):.5f}, and std: {np.std(X_test):.5f}\")\n",
    "print(f\"Train labels shape: {y_train.shape}, with distribution: {np.unique(y_train, return_counts=True)}\")\n",
    "print(f\"Test labels shape: {y_test.shape}, with distribution: {np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0317216c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field training examples 286997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_978272/693213634.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_adds = np.reshape(train_data['label'].replace('locomotion', 1).replace('no locomotion', 0).values, (-1, 1)).astype(np.float32)\n"
     ]
    }
   ],
   "source": [
    "def add_train_data(): \n",
    "    '''\n",
    "    Adds the field data for walking to the train data\n",
    "    param: train_data = training data from lab\n",
    "    param: include_field = boolean to include field data or not\n",
    "    return: train_data with field data added if include_field is True\n",
    "    '''\n",
    "    path = \"/home/elias/2025/sshfs_mounter_2025/data_elias/ECSS_2026/raw_field\"\n",
    "    \n",
    "    files = sorted(os.listdir(path))\n",
    "    df_l = []\n",
    "    \n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(path, f)) \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            df_l.append(df)\n",
    "        \n",
    "    field_data = pd.concat(df_l, ignore_index=True)\n",
    "    \n",
    "    print('field training examples', len(field_data))\n",
    "    \n",
    "    # randomly choose 16000 examples to add to training data\n",
    "    field_data_train = field_data.sample(n=16000, random_state=42).reset_index(drop=True)\n",
    "    field_data_test = field_data.sample(n=16000, random_state=22).reset_index(drop=True)\n",
    "    \n",
    "    return field_data_train, field_data_test\n",
    "\n",
    "\n",
    "train_data, field_data_test = add_train_data()\n",
    "\n",
    "train_data.drop(columns=['speed'], inplace=True)\n",
    "\n",
    "y_adds = np.reshape(train_data['label'].replace('locomotion', 1).replace('no locomotion', 0).values, (-1, 1)).astype(np.float32)\n",
    "X_adds = np.reshape(train_data.drop(columns=['label']).values, (-1, 1, 260)).astype(np.float32)\n",
    "\n",
    "# concatenate train data with adds\n",
    "X_train = np.concatenate((X_train, X_adds), axis=0)\n",
    "y_train = np.concatenate((y_train, y_adds), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "23d9c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4803\n",
      "Epoch [2/20], Loss: 0.3798\n",
      "Epoch [3/20], Loss: 0.3453\n",
      "Epoch [4/20], Loss: 0.2166\n",
      "Epoch [5/20], Loss: 0.1368\n",
      "Epoch [6/20], Loss: 0.1075\n",
      "Epoch [7/20], Loss: 0.0853\n",
      "Epoch [8/20], Loss: 0.0678\n",
      "Epoch [9/20], Loss: 0.0502\n",
      "Epoch [10/20], Loss: 0.0402\n",
      "Epoch [11/20], Loss: 0.0360\n",
      "Epoch [12/20], Loss: 0.0321\n",
      "Epoch [13/20], Loss: 0.0308\n",
      "Epoch [14/20], Loss: 0.0298\n",
      "Epoch [15/20], Loss: 0.0286\n",
      "Epoch [16/20], Loss: 0.0279\n",
      "Epoch [17/20], Loss: 0.0278\n",
      "Epoch [18/20], Loss: 0.0277\n",
      "Epoch [19/20], Loss: 0.0273\n",
      "Epoch [20/20], Loss: 0.0269\n"
     ]
    }
   ],
   "source": [
    "# load data, set batch size, shuffling\n",
    "dataset = DataLoader(DataLoaderAcc(X_train, y_train), batch_size=32, shuffle=True)\n",
    "\n",
    "# set model.train() to enable training mode\n",
    "model.train()\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, targets in dataset:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(inputs) # forward propagation\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(outputs, targets) # loss calculation\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # update weights\n",
    "        \n",
    "        # loss accumulation\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0c04cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X_test)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test) # model inference with\n",
    "binary_predictions = (torch.sigmoid(predictions) >= 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "731d6ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9981\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = (binary_predictions.numpy() == y_test.astype(np.int32)).sum()\n",
    "accuracy = correct_predictions / y_test.shape[0]\n",
    "print(f'Train Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3c3acf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_978272/174302188.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_test = np.array(field_data_test['label'].replace({'locomotion': 1, 'no locomotion': 0}).values).reshape(-1, 1).astype(np.float32)\n"
     ]
    }
   ],
   "source": [
    "y_test = np.array(field_data_test['label'].replace({'locomotion': 1, 'no locomotion': 0}).values).reshape(-1, 1).astype(np.float32)\n",
    "X_test = np.array(field_data_test.drop(columns=['label', 'speed']).values).reshape(-1, 1, 260).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d84267d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(X_test)\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test) # model inference with\n",
    "binary_predictions = (torch.sigmoid(predictions) >= 0.5).int()\n",
    "correct_predictions = (binary_predictions.numpy() == y_test.astype(np.int32)).sum()\n",
    "accuracy = correct_predictions / y_test.shape[0]\n",
    "print(f'Train Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
